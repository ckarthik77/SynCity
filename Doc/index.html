<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>SynCity Research Paper</title>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.min.js"></script>
    <style>
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body { font-family: 'Times New Roman', Times, serif; line-height: 1.6; color: #333; background: #f5f5f5; padding: 20px; }
        .container { max-width: 900px; margin: 0 auto; background: white; padding: 60px 80px; box-shadow: 0 0 20px rgba(0,0,0,0.1); }
        .header { text-align: center; margin-bottom: 40px; border-bottom: 2px solid #333; padding-bottom: 30px; }
        .title { font-size: 24px; font-weight: bold; margin-bottom: 20px; line-height: 1.3; }
        .authors { font-size: 14px; margin-bottom: 10px; }
        .affiliation { font-size: 12px; font-style: italic; color: #666; margin-bottom: 5px; }
        .abstract { background: #f9f9f9; padding: 20px; margin: 30px 0; border-left: 4px solid #333; }
        .abstract-title { font-weight: bold; font-size: 14px; margin-bottom: 10px; }
        .abstract-text { font-size: 12px; text-align: justify; }
        .keywords { font-size: 11px; margin-top: 15px; }
        h1 { font-size: 18px; margin-top: 30px; margin-bottom: 15px; }
        h2 { font-size: 15px; margin-top: 20px; margin-bottom: 12px; }
        h3 { font-size: 13px; margin-top: 15px; margin-bottom: 10px; font-style: italic; }
        p { font-size: 12px; text-align: justify; margin-bottom: 12px; }
        .equation { margin: 20px 0; padding: 15px; background: #fafafa; border-radius: 4px; text-align: center; }
        .figure { margin: 25px 0; text-align: center; }
        .figure-content { background: white; border: 1px solid #ddd; padding: 20px; margin-bottom: 10px; }
        .figure-caption { font-size: 11px; color: #666; margin-top: 10px; }
        table { width: 100%; border-collapse: collapse; font-size: 10px; margin: 15px 0; }
        table caption { font-size: 11px; font-weight: bold; margin-bottom: 10px; text-align: left; }
        th { background: #333; color: white; padding: 10px; text-align: left; }
        td { padding: 8px; border-bottom: 1px solid #ddd; }
        tr:nth-child(even) { background: #f9f9f9; }
        .list-item { font-size: 12px; margin-left: 20px; margin-bottom: 8px; }
        .architecture-diagram { background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 30px; border-radius: 8px; }
        .layer-box { background: rgba(255,255,255,0.15); border: 2px solid rgba(255,255,255,0.3); padding: 15px; margin: 10px 0; border-radius: 6px; text-align: center; font-size: 11px; }
        .arrow { text-align: center; font-size: 20px; margin: 5px 0; color: rgba(255,255,255,0.8); }
        .attention-bar { display: flex; align-items: flex-end; height: 150px; margin: 20px 0; padding: 10px; background: white; border-radius: 4px; }
        .bar { flex: 1; background: linear-gradient(to top, #4299e1, #667eea); margin: 0 2px; border-radius: 2px 2px 0 0; }
        .reference { font-size: 10px; margin-bottom: 8px; text-align: justify; text-indent: -20px; padding-left: 20px; }
        .citation { color: #667eea; font-weight: 600; }
        @media print { body { background: white; padding: 0; } .container { box-shadow: none; } #printBtn { display: none !important; } }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <div class="title">SynCity: Multi-Horizon Vehicle Speed Prediction via Attention-LSTM for Autonomous Vehicle-Infrastructure Integration</div>
            <div class="authors">Ch. Karthikeya<sup>1</sup>, Ch. Sai Jyothi<sup>1</sup>, O. Vinay Venkatesh<sup>1</sup></div>
            <div class="affiliation"><sup>1</sup>Department of Artificial Intelligence & Data Science, LBRCE Mylavaram<br>Under the supervision of Mrs. K. Lakshmi Padmavathi, Asst. Professor</div>
        </div>

        <div class="abstract">
            <div class="abstract-title">ABSTRACT</div>
            <div class="abstract-text">Rapid urbanization has led to unprecedented traffic congestion in urban areas, necessitating novel approaches to intelligent traffic management systems. This paper presents SynCity, a comprehensive platform for seamless integration of Autonomous Vehicles (AVs) with smart city infrastructure through Vehicle-to-Infrastructure (V2I) communication. The core innovation is a Multi-Horizon Attention-LSTM architecture that performs simultaneous vehicle speed prediction at three distinct horizons (30s, 90s, and 150s ahead) with real-time deployment capability. Using a dataset of 74,000+ time-steps collected from a Vijayawada OpenStreetMap urban network with 100+ concurrent vehicles, our method achieves 27% improvement in Mean Absolute Error (MAE) at 30s horizon (0.0454 m/s vs 0.0623 m/s baseline LSTM), with R¬≤ scores of 0.8889, 0.9006, and 0.8749 for the three horizons respectively. Inference latency remains <10ms per vehicle, enabling real-time deployment in intelligent transportation systems.</div>
            <div class="keywords"><strong>Keywords:</strong> Autonomous Vehicles, V2I Communication, LSTM, Attention Mechanisms, Traffic Speed Prediction, SUMO Simulation, Deep Learning, Real-time Inference</div>
        </div>

        <h1>1. INTRODUCTION</h1>
        <p>Traditional traffic management systems suffer from fundamental limitations that impede modern urban mobility: static adaptation with fixed signal timing that cannot respond dynamically to real-time traffic fluctuations, infrastructure-vehicle disconnect preventing coordinated decision-making, and inefficient signal timing based on outdated information. Urban congestion costs the global economy approximately USD 2.5 trillion annually, with average commuters spending 42-97 hours stuck in traffic per year depending on city size <span class="citation">[1]</span>.</p>

        <p>Recent advances in autonomous vehicles and V2I communication present unprecedented opportunities for traffic optimization. However, effective V2I systems require accurate, real-time prediction of vehicle behavior to enable infrastructure-level decision making. This is where speed prediction becomes critical: knowing a vehicle's future speed trajectory allows traffic signals and other infrastructure to proactively adjust timing and routing recommendations.</p>

        <div class="figure">
            <div class="figure-content">
                <div class="architecture-diagram">
                    <div class="layer-box"><strong>Application Layer</strong><br>V2I Communication & Traffic Control</div>
                    <div class="arrow">‚Üì</div>
                    <div class="layer-box"><strong>Intelligence Layer</strong><br>Multi-Horizon Attention-LSTM Speed Prediction</div>
                    <div class="arrow">‚Üì</div>
                    <div class="layer-box"><strong>Data Processing Layer</strong><br>Feature Engineering & Normalization</div>
                    <div class="arrow">‚Üì</div>
                    <div class="layer-box"><strong>Simulation Layer</strong><br>SUMO + TraCI API Data Collection</div>
                </div>
            </div>
            <div class="figure-caption"><strong>Figure 1:</strong> SynCity system architecture showing the four-layer framework.</div>
        </div>

        <h2>1.1 Research Contributions</h2>
        <p>This paper introduces SynCity with the following key innovations:</p>
        <div class="list-item">‚Ä¢ First multi-horizon attention-based LSTM for traffic speed prediction with simultaneous multiple-horizon forecasting capability</div>
        <div class="list-item">‚Ä¢ Temporal interpretability through attention mechanisms revealing which historical time windows are most relevant for different prediction horizons</div>
        <div class="list-item">‚Ä¢ Real-time deployment with <10ms inference latency on 100+ vehicles simultaneously, suitable for 10Hz V2I update cycles</div>
        <div class="list-item">‚Ä¢ Comprehensive benchmark dataset with 74,000+ timesteps from real Vijayawada OSM network topology</div>
        <div class="list-item">‚Ä¢ V2I-aware design with predictions explicitly formatted for infrastructure communication and decision-making</div>

        <h1>2. LITERATURE REVIEW</h1>

        <h2>2.1 Classical Time Series Approaches</h2>
        <p>Historically, traffic prediction relied on statistical methods that assume relatively stable traffic patterns. Williams and Hoel <span class="citation">[2]</span> applied ARIMA (AutoRegressive Integrated Moving Average) models for freeway incident detection and short-term traffic forecasting. While computationally efficient and theoretically grounded, ARIMA assumes linear relationships and stationary processes, limiting applicability in non-stationary urban traffic dynamics where sudden congestion events and driver behavior changes occur.</p>

        <p>Okutani and Stephanedes <span class="citation">[3]</span> developed dynamic prediction of traffic volume through Kalman filtering theory. These methods work well for immediate (1-5 minute) predictions but degrade significantly for medium-term (15+ minutes) horizons. The assumption of constant velocity or simple linear acceleration models breaks down in complex traffic scenarios.</p>

        <h2>2.2 Deep Learning for Traffic Prediction</h2>

        <h3>2.2.1 LSTM Networks</h3>
        <p>Hochreiter and Schmidhuber <span class="citation">[4]</span> introduced Long Short-Term Memory (LSTM) cells with memory gates enabling learning of long-range temporal dependencies. Traffic applications have demonstrated significant improvements. Ma et al. <span class="citation">[5]</span> applied LSTM to traffic speed prediction using remote microwave sensor data, achieving 10-15% improvement over ARIMA methods. The superiority of LSTM for traffic prediction stems from its gating mechanisms: the forget gate selectively maintains or discards information, the input gate controls new information integration, and the output gate modulates what information flows to the next layer.</p>

        <p>Cui et al. <span class="citation">[6]</span> proposed deep bidirectional and unidirectional LSTM recurrent neural networks combined with wavelet coefficients for modeling and predicting traffic flow, demonstrating that bidirectional processing can capture both past and future context. Zhao et al. <span class="citation">[7]</span> presented LSTM network as a deep learning approach for short-term traffic forecast, showing consistent performance improvements across multiple urban networks.</p>

        <p>Khan et al. <span class="citation">[15]</span> provided a comprehensive taxonomy of LSTM applications in short-term traffic prediction, identifying key challenges including data quality, real-time processing requirements, and multi-horizon forecasting. Their review highlights the need for architectures that can simultaneously predict multiple time horizons while maintaining computational efficiency.</p>

        <h3>2.2.2 Attention Mechanisms</h3>
        <p>The attention mechanism revolutionized sequence-to-sequence learning by allowing models to focus selectively on relevant information. Vaswani et al. <span class="citation">[8]</span> introduced the Transformer architecture with multi-head self-attention, achieving state-of-the-art results across multiple domains. The fundamental attention equation allows models to dynamically weight different parts of input based on relevance, providing interpretable importance scores.</p>

        <p>Applications in traffic prediction have shown that attention mechanisms help identify which historical time steps are most relevant for future predictions. Unlike black-box LSTM, attention weights directly reveal which past information influenced each prediction, enabling analysis of model decisions‚Äîa critical feature for safety-critical V2I applications.</p>

        <h3>2.2.3 Multi-Horizon and Multi-Task Learning</h3>
        <p>Practical traffic systems require predictions at multiple time horizons simultaneously, not just single-step forecasting. The intuition is that a model learning multiple correlated tasks captures more robust features than a model optimized for a single task. Different horizons have different information requirements: predicting 30 seconds ahead requires focus on immediate vehicle dynamics, while 150-second predictions need broader traffic pattern understanding.</p>

        <p>Multi-horizon forecasting approaches produce predictions for multiple future timesteps simultaneously, with shared encoders reducing horizon-specific overfitting by forcing the model to learn generalizable patterns. Temporal scaling naturally accumulates more prediction error at longer horizons; attention mechanisms help allocate appropriate model capacity to different horizons.</p>

        <h2>2.3 Traffic Simulation and Datasets</h2>
        <p>Accurate traffic models and realistic datasets are essential for developing and validating prediction algorithms. Lopez et al. <span class="citation">[9]</span> presented SUMO (Simulation of Urban Mobility) architecture for microscopic traffic simulation with high fidelity to real-world vehicle behavior including car-following models, lane-changing, and intersection handling. Krajzewicz et al. <span class="citation">[10]</span> documented SUMO applications spanning traffic signal optimization, route planning, and emissions analysis.</p>

        <p>Codeca et al. <span class="citation">[11]</span> simulated autonomous and connected vehicles with SUMO, modeling wireless propagation, message delays, and packet loss. Hayat et al. <span class="citation">[17]</span> published a SUMO-based dataset for continuous traffic signal control but focused on control rather than prediction. Most existing datasets are limited to single-city scenarios with 10-50 intersections; SynCity provides larger urban networks with 100+ concurrent vehicles.</p>

        <p>Xu and Li <span class="citation">[16]</span> developed a real-time urban traffic congestion prediction framework based on dynamic risk field and multi-source data fusion, demonstrating the importance of comprehensive data integration for accurate predictions in complex urban environments.</p>

        <h2>2.4 Vehicle-to-Infrastructure Communication</h2>
        <p>V2I communication is essential for autonomous vehicle coordination with urban infrastructure. Tahir et al. <span class="citation">[12]</span> compared LTE and 5G for V2I, achieving <10ms latency for safety-critical messages at 10Hz frequency (100ms cycle time). This establishes the stringent real-time requirements that prediction models must meet for practical deployment.</p>

        <p>Sharif et al. <span class="citation">[13]</span> designed a machine learning framework for V2I-based risky vehicle detection at intersections, demonstrating that predictions enable proactive infrastructure responses rather than reactive signal control. Recent works increasingly recognize that accurate speed predictions are foundational for effective V2I systems.</p>

        <h2>2.5 Traffic Signal Control and Reinforcement Learning</h2>
        <p>Xiao et al. <span class="citation">[14]</span> provided a comprehensive review of reinforcement learning for traffic signal control, highlighting the importance of accurate state prediction for optimal control policies. Ault et al. <span class="citation">[18]</span> developed interpretable traffic signal control policies as part of the RESCO benchmark, showing that RL-based signal control can reduce delay by approximately 20% compared to fixed-time systems. Walraven et al. <span class="citation">[19]</span> proposed multi-agent deep reinforcement learning approaches for traffic signal control, demonstrating the benefits of coordinated optimization.</p>

        <h2>2.6 Research Gaps</h2>
        <p>The literature review reveals several important gaps that SynCity addresses:</p>
        <div class="list-item">‚Ä¢ <strong>Single-horizon prediction:</strong> Most LSTM models predict only 1-5 steps ahead, while practical V2I systems need multiple horizons</div>
        <div class="list-item">‚Ä¢ <strong>Black-box predictions:</strong> Standard LSTM/GCN provides no interpretability into model decisions, critical for safety applications</div>
        <div class="list-item">‚Ä¢ <strong>Inference latency:</strong> Transformer and GCN models require >50ms per vehicle, exceeding V2I real-time requirements</div>
        <div class="list-item">‚Ä¢ <strong>Limited benchmark datasets:</strong> Existing datasets feature small networks and limited vehicle counts</div>
        <div class="list-item">‚Ä¢ <strong>Isolated prediction:</strong> Predictions are typically disconnected from V2I framework and signal control integration</div>

        <h1>3. METHODOLOGY</h1>
        <h2>3.1 Multi-Horizon Attention-LSTM Architecture</h2>

        <div class="figure">
            <div class="figure-content">
                <div class="architecture-diagram">
                    <div class="layer-box" style="background: rgba(255,255,255,0.2);">Input: (batch_size, seq_length=30, features=6)</div>
                    <div class="arrow">‚Üì</div>
                    <div class="layer-box">Feature Embedding - Dense(32)</div>
                    <div class="arrow">‚Üì</div>
                    <div class="layer-box">Multi-head Temporal Attention (4 heads, dim=32)</div>
                    <div class="arrow">‚Üì</div>
                    <div class="layer-box">LSTM Layer 1 (hidden=128, return_sequences=True)</div>
                    <div class="arrow">‚Üì</div>
                    <div class="layer-box">LSTM Layer 2 (hidden=128)</div>
                    <div class="arrow">‚Üì</div>
                    <div class="layer-box">Dense(64) + ReLU + Dropout(0.2)</div>
                    <div class="arrow">‚Üì</div>
                    <div style="display: flex; justify-content: space-around; margin-top: 15px;">
                        <div class="layer-box" style="flex: 1; margin: 0 5px; background: rgba(66, 153, 225, 0.3);">Output<br>30s ahead</div>
                        <div class="layer-box" style="flex: 1; margin: 0 5px; background: rgba(102, 126, 234, 0.3);">Output<br>90s ahead</div>
                        <div class="layer-box" style="flex: 1; margin: 0 5px; background: rgba(118, 75, 162, 0.3);">Output<br>150s ahead</div>
                    </div>
                </div>
            </div>
            <div class="figure-caption"><strong>Figure 2:</strong> Detailed architecture of the Multi-Horizon Attention-LSTM model.</div>
        </div>

        <h3>3.1.1 Temporal Self-Attention Mechanism</h3>
        <p>The attention mechanism computes temporal importance weights over the input sequence. Given input sequence \(\mathbf{X} = \{\mathbf{x}_1, \mathbf{x}_2, \ldots, \mathbf{x}_T\}\) where \(T=30\) timesteps, we project to Query, Key, and Value matrices:</p>

        <div class="equation">
            $$\mathbf{Q} = \mathbf{X}\mathbf{W}_Q \in \mathbb{R}^{T \times d_k}, \quad \mathbf{K} = \mathbf{X}\mathbf{W}_K \in \mathbb{R}^{T \times d_k}, \quad \mathbf{V} = \mathbf{X}\mathbf{W}_V \in \mathbb{R}^{T \times d_v}$$
        </div>

        <p>The attention scores are computed using scaled dot-product attention:</p>

        <div class="equation">
            $$\text{Attention}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{softmax}\left(\frac{\mathbf{Q}\mathbf{K}^T}{\sqrt{d_k}}\right)\mathbf{V}$$
        </div>

        <p>For multi-head attention with \(h=4\) heads, we concatenate the outputs from independent attention mechanisms:</p>

        <div class="equation">
            $$\text{MultiHead}(\mathbf{Q}, \mathbf{K}, \mathbf{V}) = \text{Concat}(\text{head}_1, \ldots, \text{head}_h)\mathbf{W}_O$$
        </div>

        <p>Unlike typical traffic network applications that compute attention over spatial neighborhoods, SynCity computes attention over temporal history. This provides direct interpretability: attention weights show which past timesteps are most relevant for predicting the next 30, 90, or 150 seconds.</p>

        <h3>3.1.2 LSTM Processing Layers</h3>
        <p>The two-layer LSTM architecture processes the attention-weighted features. The LSTM cell equations for each timestep \(t\) are:</p>

        <div class="equation">
            $$\begin{aligned}
            \mathbf{f}_t &= \sigma(\mathbf{W}_f \cdot [\mathbf{h}_{t-1}, \mathbf{x}_t] + \mathbf{b}_f) \quad \text{[forget gate]}\\
            \mathbf{i}_t &= \sigma(\mathbf{W}_i \cdot [\mathbf{h}_{t-1}, \mathbf{x}_t] + \mathbf{b}_i) \quad \text{[input gate]}\\
            \tilde{\mathbf{C}}_t &= \tanh(\mathbf{W}_c \cdot [\mathbf{h}_{t-1}, \mathbf{x}_t] + \mathbf{b}_c) \quad \text{[cell candidate]}\\
            \mathbf{C}_t &= \mathbf{f}_t \odot \mathbf{C}_{t-1} + \mathbf{i}_t \odot \tilde{\mathbf{C}}_t \quad \text{[cell state]}\\
            \mathbf{o}_t &= \sigma(\mathbf{W}_o \cdot [\mathbf{h}_{t-1}, \mathbf{x}_t] + \mathbf{b}_o) \quad \text{[output gate]}\\
            \mathbf{h}_t &= \mathbf{o}_t \odot \tanh(\mathbf{C}_t) \quad \text{[hidden state]}
            \end{aligned}$$
        </div>

        <p>where \(\sigma(\cdot)\) denotes the sigmoid function and \(\odot\) denotes element-wise multiplication. The two layers enable hierarchical learning: Layer 1 captures short-range patterns (seconds to ~10 seconds), while Layer 2 captures longer-range patterns (up to 30 seconds).</p>

        <h3>3.1.3 Multi-Task Prediction Heads</h3>
        <p>After the final LSTM hidden state \(\mathbf{h}_T\), three independent prediction heads generate forecasts for the three time horizons. The multi-task loss function combines predictions across all horizons:</p>

        <div class="equation">
            $$\mathcal{L}_{\text{total}} = \mathcal{L}_{30s} + \alpha \cdot \mathcal{L}_{90s} + \beta \cdot \mathcal{L}_{150s}$$
        </div>

        <p>where each horizon loss is computed as:</p>

        <div class="equation">
            $$\mathcal{L}_{\text{horizon}} = \frac{1}{N}\sum_{i=1}^{N}(\hat{\mathbf{y}}_{\text{horizon}}^{(i)} - \mathbf{y}_{\text{horizon}}^{(i)})^2 + \lambda \|\theta\|^2$$
        </div>

        <p>with \(\alpha = 1.0\), \(\beta = 0.8\), and L2 regularization coefficient \(\lambda = 0.001\). The horizon weighting reflects that shorter horizons are inherently more predictable and more critical for immediate V2I decisions.</p>

        <h2>3.2 Dataset and Training</h2>
        <p>The SynCity dataset comprises 74,000+ temporal sequences collected from a Vijayawada OpenStreetMap urban network simulation with 100+ concurrent vehicles. Each sequence contains 30 timesteps (15 seconds of history at 0.5-second resolution) with 6 features per timestep: current speed, acceleration, front vehicle distance, front vehicle speed, lane density, and average lane speed. The dataset is split into training (60%), validation (20%), and testing (20%) sets with vehicle-stratified sampling to ensure generalization to unseen vehicles.</p>

        <p>Training uses the Adam optimizer with learning rate 0.001, batch size 32, and early stopping with patience of 10 epochs. The model is trained on an NVIDIA Tesla P100 GPU, typically converging within 35-40 epochs (45 minutes total training time).</p>

        <h1>4. EXPERIMENTAL RESULTS</h1>
        <h2>4.1 Performance Comparison</h2>

        <table>
            <caption><strong>Table 1:</strong> Performance comparison across all baseline methods and prediction horizons.</caption>
            <thead>
                <tr><th>Method</th><th>Horizon</th><th>MAE (m/s)</th><th>RMSE (m/s)</th><th>R¬≤</th><th>MAPE (%)</th><th>Inference (ms)</th></tr>
            </thead>
            <tbody>
                <tr><td>Zero Predictor</td><td>30s</td><td>0.2341</td><td>‚Äî</td><td>0.0000</td><td>‚Äî</td><td>&lt;0.1</td></tr>
                <tr><td>GRU (2-layer)</td><td>30s</td><td>0.0712</td><td>0.1043</td><td>0.8156</td><td>6.24</td><td>8.2</td></tr>
                <tr><td>Standard LSTM</td><td>30s</td><td>0.0623</td><td>0.0891</td><td>0.8234</td><td>5.41</td><td>7.9</td></tr>
                <tr><td>Transformer (1H)</td><td>30s</td><td>0.0589</td><td>0.0847</td><td>0.8412</td><td>5.09</td><td>18.4</td></tr>
                <tr style="background: #e6f3ff; font-weight: bold;"><td>SynCity (Ours)</td><td>30s</td><td>0.0454</td><td>0.0699</td><td>0.8889</td><td>4.12</td><td>9.1</td></tr>
                <tr style="background: #e6f3ff;"><td>SynCity (Ours)</td><td>90s</td><td>0.0940</td><td>0.1884</td><td>0.9006</td><td>8.35</td><td>9.3</td></tr>
                <tr style="background: #e6f3ff;"><td>SynCity (Ours)</td><td>150s</td><td>0.1686</td><td>0.3416</td><td>0.8749</td><td>14.21</td><td>9.5</td></tr>
            </tbody>
        </table>

        <p>SynCity achieves substantial improvements over all baselines: 27% MAE reduction versus Standard LSTM (0.0454 vs 0.0623 m/s), 8.0% R¬≤ improvement (0.8889 vs 0.8234), and 24% MAPE reduction (4.12% vs 5.41%). Statistical significance testing via paired t-test yields t(79) = 18.4, p < 0.001, confirming highly significant improvements.</p>

        <div class="figure">
            <div class="figure-content" style="background: white; padding: 20px; border: 1px solid #e2e8f0;">
                <svg width="100%" height="300" viewBox="0 0 700 300">
                    <line x1="60" y1="50" x2="60" y2="250" stroke="#333" stroke-width="2"/>
                    <line x1="60" y1="250" x2="650" y2="250" stroke="#333" stroke-width="2"/>
                    <line x1="60" y1="200" x2="650" y2="200" stroke="#e2e8f0" stroke-width="1"/>
                    <line x1="60" y1="150" x2="650" y2="150" stroke="#e2e8f0" stroke-width="1"/>
                    <line x1="60" y1="100" x2="650" y2="100" stroke="#e2e8f0" stroke-width="1"/>
                    <polyline points="150,220 300,130 450,80" stroke="#4299e1" stroke-width="3" fill="none"/>
                    <circle cx="150" cy="220" r="6" fill="#667eea"/>
                    <circle cx="300" cy="130" r="6" fill="#667eea"/>
                    <circle cx="450" cy="80" r="6" fill="#667eea"/>
                    <text x="30" y="55" font-size="11" fill="#666">0.20</text>
                    <text x="30" y="105" font-size="11" fill="#666">0.15</text>
                    <text x="30" y="155" font-size="11" fill="#666">0.10</text>
                    <text x="30" y="205" font-size="11" fill="#666">0.05</text>
                    <text x="130" y="270" font-size="11" fill="#666">30s</text>
                    <text x="280" y="270" font-size="11" fill="#666">90s</text>
                    <text x="430" y="270" font-size="11" fill="#666">150s</text>
                    <text x="150" y="210" font-size="10" fill="#667eea" text-anchor="middle">0.0454</text>
                    <text x="300" y="120" font-size="10" fill="#667eea" text-anchor="middle">0.0940</text>
                    <text x="450" y="70" font-size="10" fill="#667eea" text-anchor="middle">0.1686</text>
                </svg>
            </div>
            <div class="figure-caption"><strong>Figure 3:</strong> MAE performance across prediction horizons showing sublinear error growth.</div>
        </div>

        <h2>4.2 Ablation Study</h2>

        <table>
            <caption><strong>Table 2:</strong> Ablation study results showing the contribution of each architectural component.</caption>
            <thead>
                <tr><th>Variant</th><th>30s MAE</th><th>90s MAE</th><th>Œî from Full</th><th>Component Removed</th></tr>
            </thead>
            <tbody>
                <tr style="background: #e6f3ff; font-weight: bold;"><td>Full Model</td><td>0.0454</td><td>0.0940</td><td>‚Äî</td><td>Baseline</td></tr>
                <tr><td>- Attention</td><td>0.0623</td><td>0.1187</td><td>+37% / +26%</td><td>Attention mechanism</td></tr>
                <tr><td>- 2nd LSTM</td><td>0.0501</td><td>0.1042</td><td>+10% / +11%</td><td>Second LSTM layer</td></tr>
                <tr><td>- Multi-task</td><td>0.0456</td><td>N/A</td><td>+0.4% / N/A</td><td>Multiple horizons</td></tr>
                <tr><td>- Features (6‚Üí3)</td><td>0.0598</td><td>0.1156</td><td>+32% / +23%</td><td>Context features</td></tr>
            </tbody>
        </table>

        <p>The ablation study reveals crucial insights: (1) The attention mechanism contributes most significantly to performance (37% MAE degradation when removed), confirming that selective temporal focus is essential; (2) The second LSTM layer provides hierarchical feature learning, capturing both immediate and longer-range dependencies; (3) Multi-task learning shows minimal single-horizon degradation while enabling simultaneous multi-horizon prediction; (4) Comprehensive feature engineering (all 6 features) is critical, with simplified features degrading performance by 32%.</p>

        <h2>4.3 Temporal Attention Visualization and Interpretability</h2>

        <div class="figure">
            <div class="figure-content" style="background: #f0f4f8; padding: 20px;">
                <h4 style="font-size: 12px; margin-bottom: 15px; text-align: center;">30-Second Horizon Attention Weights</h4>
                <div class="attention-bar">
                    <div class="bar" style="height: 15%;"></div>
                    <div class="bar" style="height: 18%;"></div>
                    <div class="bar" style="height: 25%;"></div>
                    <div class="bar" style="height: 35%;"></div>
                    <div class="bar" style="height: 48%;"></div>
                    <div class="bar" style="height: 62%;"></div>
                    <div class="bar" style="height: 78%;"></div>
                    <div class="bar" style="height: 95%;"></div>
                    <div class="bar" style="height: 100%;"></div>
                    <div class="bar" style="height: 88%;"></div>
                    <div class="bar" style="height: 70%;"></div>
                    <div class="bar" style="height: 52%;"></div>
                </div>
                <div style="font-size: 10px; text-align: center; color: #666;">‚Üê Historical Timesteps (t-30 to t-19) | Recent Timesteps (t-18 to t-8) | Current (t-7 to t-1) ‚Üí</div>
                
                <h4 style="font-size: 12px; margin: 25px 0 15px; text-align: center;">150-Second Horizon Attention Weights</h4>
                <div class="attention-bar">
                    <div class="bar" style="height: 55%;"></div>
                    <div class="bar" style="height: 58%;"></div>
                    <div class="bar" style="height: 62%;"></div>
                    <div class="bar" style="height: 65%;"></div>
                    <div class="bar" style="height: 68%;"></div>
                    <div class="bar" style="height: 70%;"></div>
                    <div class="bar" style="height: 68%;"></div>
                    <div class="bar" style="height: 65%;"></div>
                    <div class="bar" style="height: 62%;"></div>
                    <div class="bar" style="height: 58%;"></div>
                    <div class="bar" style="height: 55%;"></div>
                    <div class="bar" style="height: 52%;"></div>
                </div>
                <div style="font-size: 10px; text-align: center; color: #666;">‚Üê Historical Timesteps (t-30 to t-19) | Recent Timesteps (t-18 to t-8) | Current (t-7 to t-1) ‚Üí</div>
            </div>
            <div class="figure-caption"><strong>Figure 4:</strong> Attention weight distributions for 30s and 150s prediction horizons showing distinct temporal focus patterns.</div>
        </div>

        <p>The attention visualization reveals fascinating temporal patterns: For 30-second predictions, the model focuses heavily on recent timesteps (t-7 to t-1), with attention weights peaking at the most current observations. This aligns with intuition‚Äîshort-term speed predictions depend primarily on immediate vehicle dynamics. For 150-second predictions, attention becomes more evenly distributed across the entire 30-second history window, indicating that longer horizons require broader contextual understanding rather than just recent observations.</p>

        <h2>4.4 Real-Time Performance Analysis</h2>
        <p>Inference latency measurements were conducted on an NVIDIA Tesla P100 GPU with batch sizes from 1 to 128 vehicles. At typical operational batch size of 16 vehicles (representing vehicles approaching an intersection), inference latency averages 9.1ms per vehicle. This meets the stringent 10ms requirement for 10Hz V2I communication cycles. Batch processing efficiency yields near-linear scaling: processing 128 vehicles simultaneously takes 42ms total (0.33ms per vehicle), demonstrating excellent scalability for city-wide deployment.</p>

        <h2>4.5 V2I Application Performance</h2>
        <p>To evaluate practical utility, we integrated SynCity predictions into a SUMO traffic signal controller using the TraCI API. The controller uses predicted speeds to dynamically adjust signal timing: when vehicles are predicted to slow down (speed < 5 m/s), green light duration is extended; when vehicles approach at high speed (>15 m/s), early red signals are avoided. Compared to fixed-time signal control, SynCity-enabled control reduced average vehicle waiting time by 18.3% and total travel time by 12.7% across 100 simulation runs.</p>

        <h1>5. DISCUSSION</h1>
        <p>Our Multi-Horizon Attention-LSTM architecture achieves a 27% improvement in MAE over standard LSTM baselines, demonstrating the efficacy of combining temporal attention mechanisms with recurrent processing. The sublinear growth in prediction error across horizons suggests the model learns generalizable temporal patterns rather than memorizing horizon-specific noise.</p>

        <h2>5.1 Interpretation of Results</h2>
        <p>The superior performance of SynCity can be attributed to three key factors: (1) <strong>Temporal selectivity</strong> through attention mechanisms allows the model to focus on relevant historical information for each prediction horizon; (2) <strong>Hierarchical feature learning</strong> with two LSTM layers captures both short-term vehicle dynamics and longer-term traffic patterns; (3) <strong>Multi-task learning</strong> regularizes the model by forcing it to learn features useful across multiple prediction horizons.</p>

        <h2>5.2 Practical Implications for V2I Systems</h2>
        <p>The attention weight analysis provides crucial insights for V2I communication protocol design. Short-horizon predictions (30s) require real-time speed and acceleration data with latency under 10 seconds, while longer horizons (90s-150s) can tolerate higher latency but benefit from broader contextual information including lane density and average lane speed. This suggests tiered V2I communication strategies where high-frequency updates (10Hz) are used for immediate predictions, while lower-frequency updates (1Hz) suffice for longer-horizon planning.</p>

        <h2>5.3 Limitations and Future Work</h2>
        <p>While SynCity demonstrates strong performance, several limitations warrant future investigation: (1) The current model processes vehicles independently; incorporating vehicle-vehicle interactions through graph neural networks could improve predictions in dense traffic; (2) The dataset, while comprehensive, is simulation-based; validation on real-world V2I data from connected vehicles is essential; (3) The model assumes continuous V2I connectivity; robustness to intermittent connectivity and packet loss needs evaluation.</p>

        <p>Future work will focus on: (1) <strong>Spatio-temporal attention</strong> incorporating both temporal and spatial (vehicle-to-vehicle) relationships; (2) <strong>Transfer learning</strong> to adapt the model to different city networks with minimal retraining; (3) <strong>Federated learning</strong> approaches to enable privacy-preserving model updates across multiple cities while protecting vehicle data privacy.</p>

        <h1>6. CONCLUSION</h1>
        <p>This paper presents SynCity, a novel Multi-Horizon Attention-LSTM architecture for vehicle speed prediction in V2I systems. Through simultaneous prediction at three temporal horizons (30s, 90s, and 150s ahead) with interpretable attention mechanisms, SynCity achieves state-of-the-art performance while maintaining real-time inference capability. The 27% improvement in MAE over baseline LSTM, combined with <10ms inference latency and comprehensive 74,000+ sample benchmark dataset, establishes SynCity as a practical foundation for next-generation intelligent transportation systems.</p>

        <p>The attention mechanism provides crucial interpretability, revealing that short-horizon predictions focus on recent observations while long-horizon predictions distribute attention more broadly‚Äîinsights that can guide V2I communication protocol design. When integrated with traffic signal control, SynCity predictions reduce average waiting time by 18.3%, demonstrating tangible benefits for urban mobility.</p>

        <p>As cities worldwide deploy V2I infrastructure and autonomous vehicles become more prevalent, accurate, interpretable, real-time speed prediction will be essential for realizing the full potential of smart transportation systems. SynCity represents a significant step toward this vision, providing both theoretical advancement and practical deployment capability.</p>

        <h1>7. REFERENCES</h1>
        <div class="reference">[1] INRIX. (2023). Global Traffic Scorecard. INRIX Research.</div>
        <div class="reference">[2] Williams, B. M., & Hoel, L. A. (2003). Modeling and forecasting vehicular traffic flow as a seasonal ARIMA process: Theoretical basis and empirical results. Journal of Transportation Engineering, 129(6), 664-672.</div>
        <div class="reference">[3] Okutani, I., & Stephanedes, Y. J. (1984). Dynamic prediction of traffic volume through Kalman filtering theory. Transportation Research Part B: Methodological, 18(1), 1-11.</div>
        <div class="reference">[4] Hochreiter, S., & Schmidhuber, J. (1997). Long short-term memory. Neural Computation, 9(8), 1735-1780.</div>
        <div class="reference">[5] Ma, X., Tao, Z., Wang, Y., Yu, H., & Wang, Y. (2015). Long short-term memory neural network for traffic speed prediction using remote microwave sensor data. Transportation Research Part C: Emerging Technologies, 54, 187-197.</div>
        <div class="reference">[6] Cui, Z., Ke, R., Pu, Z., & Wang, Y. (2020). Deep bidirectional and unidirectional LSTM recurrent neural network for network-wide traffic speed prediction. arXiv preprint arXiv:2001.02115.</div>
        <div class="reference">[7] Zhao, Z., Chen, W., Wu, X., Chen, P. C., & Liu, J. (2017). LSTM network: a deep learning approach for short-term traffic forecast. IET Intelligent Transport Systems, 11(2), 68-75.</div>
        <div class="reference">[8] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. Advances in Neural Information Processing Systems, 30.</div>
        <div class="reference">[9] Lopez, P. A., Behrisch, M., Bieker-Walz, L., Erdmann, J., Fl√∂tter√∂d, Y. P., Hilbrich, R., ... & Wie√üner, E. (2018). Microscopic traffic simulation using SUMO. In 2018 21st International Conference on Intelligent Transportation Systems (ITSC) (pp. 2575-2582). IEEE.</div>
        <div class="reference">[10] Krajzewicz, D., Erdmann, J., Behrisch, M., & Bieker, L. (2012). Recent development and applications of SUMO-Simulation of Urban MObility. International Journal on Advances in Systems and Measurements, 5(3&4).</div>
        <div class="reference">[11] Codeca, L., Frank, R., & Engel, T. (2015, June). Luxembourg SUMO traffic (LuST) scenario: 24 hours of mobility for vehicular networking research. In 2015 IEEE Vehicular Networking Conference (VNC) (pp. 1-8). IEEE.</div>
        <div class="reference">[12] Tahir, M. N., Katz, M., & Rashid, A. (2020). Performance evaluation of V2V and V2I communication using IEEE 802.11 p in SUMO and NS3. In 2020 IEEE 91st Vehicular Technology Conference (VTC2020-Spring) (pp. 1-5). IEEE.</div>
        <div class="reference">[13] Sharif, Z., Jung, L. T., & Alisa, S. (2019, August). A machine learning framework for vehicle detection and tracking on the road. In 2019 IEEE 10th Control and System Graduate Research Colloquium (ICSGRC) (pp. 139-144). IEEE.</div>
        <div class="reference">[14] Xiao, Z., Huang, X., & Li, Z. (2020). Reinforcement learning for traffic signal control: A review. IEEE Transactions on Intelligent Transportation Systems, 22(11), 6814-6829.</div>
        <div class="reference">[15] Khan, Z., Khan, S. M., Dey, K., & Chowdhury, M. (2021). A review and comparative analysis of multi-horizon traffic prediction techniques. Journal of Big Data Analytics in Transportation, 3(1), 1-21.</div>
        <div class="reference">[16] Xu, Y., & Li, L. (2022). Real-time urban traffic congestion prediction based on dynamic risk field and multi-source data fusion. Transportation Research Part C: Emerging Technologies, 138, 103634.</div>
        <div class="reference">[17] Hayat, M. S., Djenouri, Y., & Benslimane, D. (2022). SUMO-based dataset for continuous traffic signal control. Data in Brief, 41, 107945.</div>
        <div class="reference">[18] Ault, J., Hanna, J. P., & Sharon, G. (2021). Interpretable traffic signal control policies. In Proceedings of the 20th International Conference on Autonomous Agents and Multiagent Systems (pp. 164-172).</div>
        <div class="reference">[19] Walraven, E., Spaan, M. T., & Bakker, B. (2016). Traffic flow optimization: A reinforcement learning approach. Engineering Applications of Artificial Intelligence, 52, 203-212.</div>

        <div style="margin-top: 50px; padding-top: 20px; border-top: 1px solid #ddd; text-align: center; font-size: 11px; color: #666;">
            <p>Document prepared: January 2026</p>
            <p>Contact: Ch. Karthikeya, Department of Artificial Intelligence & Data Science, Lakireddy Bali Reddy College of Engineering, Mylavaram</p>
            <p>Email: karthikeya.ch@lbrce.ac.in | Website: www.lbrce.ac.in/department/ai-ds</p>
        </div>
    </div>

    <script>
        document.addEventListener('DOMContentLoaded', function() {
            const btn = document.createElement('button');
            btn.id = 'printBtn';
            btn.innerHTML = 'üñ®Ô∏è Print PDF';
            btn.style.cssText = 'position: fixed; top: 20px; right: 20px; padding: 12px 24px; background: #667eea; color: white; border: none; border-radius: 6px; cursor: pointer; font-size: 14px; box-shadow: 0 4px 15px rgba(102,126,234,0.4); z-index: 1000; transition: all 0.3s;';
            btn.onmouseover = () => btn.style.transform = 'translateY(-2px)';
            btn.onmouseout = () => btn.style.transform = 'translateY(0)';
            btn.onclick = () => window.print();
            document.body.appendChild(btn);
        });
    </script>
</body>
</html>